"""
RL Agent - SEO Action Optimizer
================================

Reinforcement Learning agent that recommends optimal SEO actions.
Uses Deep Q-Network (DQN) to learn which actions improve rankings.

In production: Learns from user outcomes over time.
Initial mode: Uses heuristic Q-values based on SEO best practices.
"""

import logging
import numpy as np
from typing import Dict, Any, List, Tuple

logger = logging.getLogger(__name__)

# Try importing heavy dependencies
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    DEEP_MODE = True
except ImportError:
    DEEP_MODE = False
    # Provide dummy classes to avoid NameErrors during import
    class torch:
        class Tensor: pass
        @staticmethod
        def no_grad(): return lambda x: x
        @staticmethod
        def FloatTensor(*args, **kwargs): return None
    class nn:
        class Module: pass
        @staticmethod
        def Linear(*args, **kwargs): return None
        @staticmethod
        def Sequential(*args, **kwargs): return None
        @staticmethod
        def Dropout(*args, **kwargs): return None
    F = None
    logger.info("RL Agent running in heuristic mode")


# Define SEO actions
SEO_ACTIONS = {
    0: {
        "id": "increase_content_500",
        "name": "Add 500 Words",
        "description": "Expand content by 500 words with relevant information",
        "impact": "high",
        "effort": "medium"
    },
    1: {
        "id": "increase_content_1000",
        "name": "Add 1000 Words",
        "description": "Major content expansion with comprehensive coverage",
        "impact": "very_high",
        "effort": "high"
    },
    2: {
        "id": "add_images_3",
        "name": "Add 3 Images",
        "description": "Add 3 relevant images with optimized alt text",
        "impact": "medium",
        "effort": "low"
    },
    3: {
        "id": "optimize_title",
        "name": "Optimize Title Tag",
        "description": "Rewrite title with keyword at front, under 60 chars",
        "impact": "high",
        "effort": "low"
    },
    4: {
        "id": "improve_meta",
        "name": "Improve Meta Description",
        "description": "Write compelling meta with CTA, 150-160 chars",
        "impact": "medium",
        "effort": "low"
    },
    5: {
        "id": "add_faq_section",
        "name": "Add FAQ Section",
        "description": "Add 5-10 FAQs answering People Also Ask questions",
        "impact": "high",
        "effort": "medium"
    },
    6: {
        "id": "add_schema_faq",
        "name": "Add FAQ Schema",
        "description": "Implement FAQ JSON-LD for rich snippets",
        "impact": "high",
        "effort": "low"
    },
    7: {
        "id": "add_schema_article",
        "name": "Add Article Schema",
        "description": "Implement Article/BlogPosting schema",
        "impact": "medium",
        "effort": "low"
    },
    8: {
        "id": "improve_internal_links",
        "name": "Add Internal Links",
        "description": "Add 5-10 relevant internal links to related content",
        "impact": "high",
        "effort": "medium"
    },
    9: {
        "id": "add_external_citations",
        "name": "Add External Citations",
        "description": "Link to 3-5 authoritative external sources",
        "impact": "medium",
        "effort": "low"
    },
    10: {
        "id": "optimize_h2_structure",
        "name": "Optimize Headings",
        "description": "Add or improve H2s with semantic keywords",
        "impact": "high",
        "effort": "medium"
    },
    11: {
        "id": "improve_readability",
        "name": "Improve Readability",
        "description": "Shorten sentences, add bullet points, simplify",
        "impact": "medium",
        "effort": "medium"
    },
    12: {
        "id": "add_video",
        "name": "Add Video Content",
        "description": "Embed relevant video to increase dwell time",
        "impact": "high",
        "effort": "medium"
    },
    13: {
        "id": "optimize_images",
        "name": "Optimize Images",
        "description": "Compress images, add alt text, use WebP format",
        "impact": "medium",
        "effort": "low"
    },
    14: {
        "id": "improve_core_vitals",
        "name": "Fix Core Web Vitals",
        "description": "Optimize LCP, CLS, FID for better performance",
        "impact": "high",
        "effort": "high"
    },
    15: {
        "id": "add_table_of_contents",
        "name": "Add Table of Contents",
        "description": "Add jump links for better navigation",
        "impact": "medium",
        "effort": "low"
    },
    16: {
        "id": "update_publish_date",
        "name": "Update Content Date",
        "description": "Refresh content with current year data",
        "impact": "medium",
        "effort": "low"
    },
    17: {
        "id": "add_author_bio",
        "name": "Add Author Bio",
        "description": "Add author credentials for E-E-A-T signals",
        "impact": "high",
        "effort": "low"
    },
    18: {
        "id": "mobile_optimization",
        "name": "Mobile Optimization",
        "description": "Improve mobile layout and tap targets",
        "impact": "high",
        "effort": "medium"
    },
    19: {
        "id": "competitor_gap_fill",
        "name": "Fill Competitor Gaps",
        "description": "Add topics competitors cover that you don't",
        "impact": "very_high",
        "effort": "high"
    }
}


class DQNNetwork(nn.Module):
    """Deep Q-Network for action value estimation."""
    
    def __init__(self, state_dim: int = 150, action_dim: int = 20):
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state):
        return self.network(state)


class SEOAgent:
    """
    Reinforcement Learning agent for SEO optimization.
    
    Deep Mode: Uses trained DQN network
    Heuristic Mode: Uses rule-based Q-value estimation
    """
    
    def __init__(self, state_dim: int = 150, action_dim: int = 20):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.model = None
        
        if DEEP_MODE:
            try:
                self.model = DQNNetwork(state_dim, action_dim)
                # Initialize with Xavier weights
                for m in self.model.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.xavier_uniform_(m.weight)
                        nn.init.zeros_(m.bias)
                logger.info("RL Agent: Deep mode initialized")
            except Exception as e:
                logger.warning(f"Could not initialize DQN: {e}")
    
    def get_optimal_actions(
        self, 
        page_state: np.ndarray, 
        gaps: Dict[str, Any],
        n_actions: int = 5
    ) -> List[Dict[str, Any]]:
        """
        Get top N recommended actions based on current page state.
        
        Args:
            page_state: Feature vector from feature_extractor
            gaps: Gap analysis results
            n_actions: Number of actions to recommend
        
        Returns:
            List of action recommendations with expected impact
        """
        if DEEP_MODE and self.model:
            return self._deep_recommend(page_state, gaps, n_actions)
        else:
            return self._heuristic_recommend(page_state, gaps, n_actions)
    
    def _deep_recommend(
        self, 
        state: np.ndarray, 
        gaps: Dict[str, Any],
        n_actions: int
    ) -> List[Dict[str, Any]]:
        """Use DQN to predict action values."""
        try:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0)
                q_values = self.model(state_tensor).squeeze(0)
                
                # Get top actions
                top_indices = q_values.topk(n_actions * 2).indices.tolist()
                
                # Filter and format recommendations
                recommendations = []
                for idx in top_indices:
                    if len(recommendations) >= n_actions:
                        break
                    
                    action = SEO_ACTIONS.get(idx)
                    if action:
                        # Adjust Q-value based on gaps
                        adjusted_q = self._adjust_q_for_gaps(
                            q_values[idx].item(), 
                            action, 
                            gaps
                        )
                        
                        recommendations.append({
                            **action,
                            "q_value": float(adjusted_q),
                            "priority": len(recommendations) + 1,
                            "expected_impact": self._q_to_impact(adjusted_q)
                        })
                
                return recommendations
                
        except Exception as e:
            logger.warning(f"Deep recommendation failed: {e}")
            return self._heuristic_recommend(state, gaps, n_actions)
    
    def _heuristic_recommend(
        self, 
        state: np.ndarray, 
        gaps: Dict[str, Any],
        n_actions: int
    ) -> List[Dict[str, Any]]:
        """
        Rule-based action recommendation.
        Analyzes state to determine best actions.
        """
        # WORLD-CLASS: Active Baseline Strategy (ABS)
        # Instead of zeros, we start with a foundation of SEO best practices
        q_values = np.array([0.4] * self.action_dim) 
        
        # Extract state features
        seo_score = state[0] * 100 if len(state) > 0 else 50
        content_score = state[1] * 100 if len(state) > 1 else 50
        aeo_score = state[5] * 100 if len(state) > 5 else 50
        geo_score = state[9] * 100 if len(state) > 9 else 50
        word_count_ratio = state[15] if len(state) > 15 else 0.3
        
        # Priority gaps from gap analysis
        priority_fixes = gaps.get('priority_fixes', [])
        
        # Calculate heuristic Q-values based on page state
        
        # Content actions
        if word_count_ratio < 0.3:  # Under 1500 words
            q_values[0] = 0.9  # Add 500 words
            q_values[1] = 0.95  # Add 1000 words
        elif word_count_ratio < 0.5:
            q_values[0] = 0.7
        
        # Title/Meta actions
        title_score = state[1] * 100 if len(state) > 1 else 50
        if title_score < 70:
            q_values[3] = 0.85  # Optimize title
        if content_score < 60:
            q_values[4] = 0.7  # Improve meta
        
        # Schema actions
        if aeo_score < 60:
            q_values[5] = 0.8  # Add FAQ section
            q_values[6] = 0.85  # Add FAQ schema
        if geo_score < 50:
            q_values[7] = 0.75  # Add Article schema
        
        # Structure actions
        q_values[8] = 0.7  # Internal links always valuable
        q_values[10] = 0.75 if content_score < 70 else 0.5  # H2 structure
        
        # E-E-A-T actions
        if geo_score < 60:
            q_values[17] = 0.8  # Author bio
            q_values[9] = 0.7  # External citations
        
        # Performance actions
        perf_score = state[3] * 100 if len(state) > 3 else 50
        if perf_score < 50:
            q_values[14] = 0.9  # Core Web Vitals
        
        # Competitor gap filling always high priority
        q_values[19] = 0.85
        
        # Boost based on actual gaps
        for gap in priority_fixes[:3]:
            gap_type = gap.get('type', '')
            if 'content' in gap_type:
                q_values[0] += 0.1
                q_values[1] += 0.1
            elif 'title' in gap_type:
                q_values[3] += 0.15
            elif 'schema' in gap_type:
                q_values[6] += 0.1
                q_values[7] += 0.1
        
        # Get top actions
        top_indices = np.argsort(q_values)[::-1][:n_actions]
        
        recommendations = []
        for rank, idx in enumerate(top_indices):
            action = SEO_ACTIONS.get(idx)
            if action and q_values[idx] > 0.3:
                recommendations.append({
                    **action,
                    "q_value": float(q_values[idx]),
                    "priority": rank + 1,
                    "expected_impact": self._q_to_impact(q_values[idx])
                })
        
        return recommendations
    
    def _adjust_q_for_gaps(
        self, 
        q: float, 
        action: Dict, 
        gaps: Dict
    ) -> float:
        """Adjust Q-value based on identified gaps."""
        boost = 0.0
        
        gap_types = [g.get('type', '') for g in gaps.get('priority_fixes', [])]
        
        if action['id'] == 'increase_content_500' and 'content' in str(gap_types):
            boost = 0.2
        elif action['id'] == 'add_faq_section' and 'aeo' in str(gap_types):
            boost = 0.15
        elif action['id'] == 'add_author_bio' and 'authority' in str(gap_types):
            boost = 0.2
        
        return min(q + boost, 1.0)
    
    def _q_to_impact(self, q: float) -> str:
        """Convert Q-value to human-readable impact string."""
        if q >= 0.85:
            return "ðŸ”¥ Critical - Major ranking improvement expected"
        elif q >= 0.7:
            return "â­ High - Significant positive impact"
        elif q >= 0.5:
            return "âœ… Medium - Noticeable improvement"
        else:
            return "ðŸ“ˆ Low - Minor optimization"


# Singleton instance
seo_agent = SEOAgent()
